{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoacirChrist/phpexcel/blob/master/09_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSKO1ayItVfT"
      },
      "source": [
        "#Geração de Texto\n",
        "Redes neurais profundas (RNP) são uma técnica de aprendizado profundo que utiliza redes neurais com várias camadas ocultas para processar dados complexos. Essas redes são capazes de aprender padrões e relações semânticas em grandes conjuntos de dados, o que as torna muito úteis para tarefas de processamento de linguagem natural (PLN).\n",
        "\n",
        "A tarefa de geração de texto utilizando redes neurais profundas (RNP) é um processo em que as redes neurais são treinadas para gerar texto coerente e relevante em resposta a entradas específicas. Isso é alcançado por meio do pré-treinamento com grandes volumes de texto, que permite às redes aprender padrões e relações semânticas entre as palavras.\n",
        "\n",
        "Uma das formas mais simples de se realizar essa tarefa é treinando uma Rede Neural para, com base em um conjunto de palavras, prever qual será a próxima palavra no texto. Por exemplo, nós poderíamos treinar uma rede para dada a entrada \"O cachorro gosta de\", a rede nos retornasse a palavra \"latir\" ou \"pular\", por exemplo.\n",
        "\n",
        "Na aula da semana, nós vamos implementar uma Rede Neural Profunda, utilizando uma arquitetura similar à utilizada na aula passada. No entanto, vamos adaptar nossa rede para gerar textos.\n",
        "\n",
        "Essa adaptação envolve, principalmente, substituir a última camada da nossa rede por uma camada com ativação pela função Softmax. Essa camada vai possuir um neurônio para cada palavra que desejamos que possa ser gerada pelo nosso sistema. Sua saída vai representar a probabilidade de que aquela palavra possa ser incluída no texto gerado.\n",
        "\n",
        "A funcionalidade que vamos implementar vai ser similar à funcionalidade de autocomplete, no entanto com textos que vamos dar como entrada para o nosso modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AonfYp3dtZJa"
      },
      "source": [
        "##Acquisição do córpus\n",
        "\n",
        "O córpus que vamos utilizar nesse caderno é um conjunto de regulamentos da nossa universidade disponíveis na seguinte URL:\n",
        "[Regulamentos da UTFPR](https://www.utfpr.edu.br/documentos/graduacao-e-educacao-profissional/prograd/diretrizes-e-regulamentos).\n",
        "\n",
        "Inicialmente, nós vamos implementar um script para fazer o download desses regulamentos disponibilizados em um servidor separado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-hlUZjStQ06",
        "outputId": "45af0c8c-0eb0-49f0-de0b-c370ba035788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download realizado e arquivo extraído no Runtime... Tudo OK\n",
            "Download realizado e arquivo extraído no Runtime... Tudo OK\n",
            "Download realizado e arquivo extraído no Runtime... Tudo OK\n",
            "Download realizado e arquivo extraído no Runtime... Tudo OK\n",
            "Download realizado e arquivo extraído no Runtime... Tudo OK\n",
            "Download realizado e arquivo extraído no Runtime... Tudo OK\n",
            "Download realizado e arquivo extraído no Runtime... Tudo OK\n",
            "Download realizado e arquivo extraído no Runtime... Tudo OK\n"
          ]
        }
      ],
      "source": [
        "import io, tarfile, requests, os, pandas as pd\n",
        "\n",
        "\n",
        "# download the dataset\n",
        "def download (url, filename=''):\n",
        "  if (os.path.isfile(filename)):\n",
        "    print('Arquivo já existente no Runtime... Tudo OK')\n",
        "    return\n",
        "  response = requests.get(url)\n",
        "  with open(f'./{filename}', 'wb') as f:\n",
        "      f.write(response.content)\n",
        "      print('Download realizado e arquivo extraído no Runtime... Tudo OK')\n",
        "\n",
        "urls = [\n",
        "  \"https://raw.githubusercontent.com/watinha/nlp-text-mining-datasets/main/regulamentos/aa-ab-cf-dispensa-2021.html\",\n",
        "  \"https://raw.githubusercontent.com/watinha/nlp-text-mining-datasets/main/regulamentos/ac-2022.html\",\n",
        "  \"https://raw.githubusercontent.com/watinha/nlp-text-mining-datasets/main/regulamentos/diretrizes-grad-2022.html\",\n",
        "  \"https://raw.githubusercontent.com/watinha/nlp-text-mining-datasets/main/regulamentos/ead-2022.html\",\n",
        "  \"https://raw.githubusercontent.com/watinha/nlp-text-mining-datasets/main/regulamentos/estagio-2020.html\",\n",
        "  \"https://raw.githubusercontent.com/watinha/nlp-text-mining-datasets/main/regulamentos/extensao-2022.html\",\n",
        "  \"https://raw.githubusercontent.com/watinha/nlp-text-mining-datasets/main/regulamentos/rodp-2019.html\",\n",
        "  \"https://raw.githubusercontent.com/watinha/nlp-text-mining-datasets/main/regulamentos/tcc-2022.html\"\n",
        "]\n",
        "\n",
        "filenames = []\n",
        "\n",
        "for url in urls:\n",
        "  filename = url.split('/')[-1]\n",
        "  filenames.append(filename)\n",
        "  download(url, filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sd69FgEdsBh"
      },
      "source": [
        "Esses arquivos são disponibilizados como páginas HTML. No entanto, para simplificar nossa abordagem de geração de textos, nós podemos remover a estrutura HTML e conteúdos que não são relevantes para gerar os textos.\n",
        "\n",
        "Nesse contexto, nós podemos utilizar a biblioteca [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) e localizar apenas os conteúdos textuais relevantes dentro do código das páginas HTML.\n",
        "\n",
        "Nesses regulamentos, de forma simplificada, podemos considerar que os textos mais relevantes estão localizados dentro de elementos HTML P (parágrafo) com a classe \"Texto_Justificado\". Por isso, utilizamos a biblioteca Beautiful Soup para localizar esses elementos e extrair os conteúdos textuais de dentro deles para gerarmos o nosso córpus de estudo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwCi-FxtvL47",
        "outputId": "0c1f305c-9345-40b3-d230-ea9e15a92e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ANEXO DA RESOLUÇÃO COGEP/UTFPR Nº 110, DE 19 DE O\n",
            " -   \n",
            " -  Regulamento para \n",
            "as atividades acompanhadas, o a\n",
            " -   \n",
            " -   \n",
            " -  Capítulo I\n",
            " -  Das Atividades Acompanhadas\n",
            " - Art. 1º  As atividades acompanhadas \n",
            "caracterizam-\n",
            " - Art. 2º  Poderão solicitar a \n",
            "realização de ativid\n",
            " - Art. 2º  Poderão solicitar a \n",
            "realização de ativid\n"
          ]
        }
      ],
      "source": [
        "import codecs\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for filename in filenames:\n",
        "  with codecs.open(filename, encoding='cp1252') as f:\n",
        "    html = f.read()\n",
        "    soup = BeautifulSoup(html)\n",
        "    ps = soup.select('div[unselectable=on] ~ p')\n",
        "    article = ''\n",
        "\n",
        "    for p in ps:\n",
        "      if p.get_text().lower().startswith('art.'):\n",
        "        article = p.get_text()\n",
        "        corpus.append(article)\n",
        "      else:\n",
        "        paragraph = p.get_text()\n",
        "        corpus.append(f'{article} {paragraph}')\n",
        "\n",
        "\n",
        "print('\\n - '.join([ doc[:50] for doc in corpus[:10]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VboMmvEZ78Wj"
      },
      "source": [
        "##Construção do Dataset\n",
        "Para construir nosso dataset e simplificar nossos experimentos, vamos limpar caractéres desconhecidos e pontuações. No código a seguir, executamos a remoção de diferentes tipos de pontuações e formas de definir espaços em textos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yO7GG-Bs7--6",
        "outputId": "8537de19-b132-4e7a-c5dc-652dc971a5f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anexo da resolução cogep/utfpr nº 110 de 19 de out\n",
            " - \n",
            " - regulamento para as atividades acompanhadas o abon\n",
            " - \n",
            " - \n",
            " - capítulo i\n",
            " - das atividades acompanhadas\n",
            " - art. 1º as atividades acompanhadas caracterizam se\n",
            " - art. 2º poderão solicitar a realização de atividad\n",
            " - art. 2º poderão solicitar a realização de atividad\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def clean(doc):\n",
        "  words = doc.split()\n",
        "  chars_to_replace = '!\"#$%&\\'()*+,-:;<=>?@[\\\\]^_`{|}~'\n",
        "  table = doc.maketrans(chars_to_replace, ' ' * len(chars_to_replace))\n",
        "  cleaned_words = [w.translate(table) for w in words]\n",
        "  cleaned_doc = ' '.join(cleaned_words)\n",
        "  cleaned_doc = cleaned_doc.replace(u'\\xa0', u' ')\n",
        "  cleaned_doc = cleaned_doc.replace(u'\\u200b', u' ')\n",
        "  cleaned_doc = cleaned_doc.replace(u'\\n', u' ')\n",
        "  cleaned_doc = re.sub(r'\\s+', ' ', cleaned_doc)\n",
        "  cleaned_doc = cleaned_doc.lower().lstrip()\n",
        "\n",
        "  return cleaned_doc\n",
        "\n",
        "\n",
        "corpus = [ clean(doc) for doc in corpus ]\n",
        "print('\\n - '.join([ doc[:50] for doc in corpus[:10]]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odETYR3rAnjw"
      },
      "source": [
        "No momento, nós temos um conjunto de strings armazenadas na nossa variável corpus. Para definir o nosso dataset, vamos precisar separar esse texto em entrada e classe (label). Para isso, vamos precisar \"quebrar\" o nosso texto em palavras.\n",
        "\n",
        "Para implementar essa funcionalidade, vamos utilizar o SPACY para separar o nosso texto em tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO5gHb22TUjc",
        "outputId": "0adda2c4-35ac-4147-d836-3ccf13e80f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: blis, thinc, spacy\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.2.0 spacy-3.8.4 thinc-8.3.4\n",
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "#!pip install --upgrade spacy\n",
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5KW7pVB4J9"
      },
      "source": [
        "Com a biblioteca SPACY carregada e dado um tamanho de janela de análise, vamos construir as linhas do nosso dataset.\n",
        "\n",
        "O tamanho da janela de análise representa o número de palavras/tokens que a nossa rede neural vai receber como entrada para tentar prever a próxima palavra. Logo, para definir cada linha do nosso dataset, nós vamos definir cada linha/amostra com uma entrada de N (tamanho da janela) palavras/tokens e a classe dessa linha vai ser a próxima palavra.\n",
        "\n",
        "Vamos implementar uma estratégia de algorítmo de janela deslizante para construir esse dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ6cX0uBlJPO",
        "outputId": "31d79a72-3536-4296-ece4-cb451c6d4da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(49520,)\n",
            "['' 'anexo' 'anexo da' 'anexo da resolução' 'anexo da resolução cogep'\n",
            " 'anexo da resolução cogep /' 'anexo da resolução cogep / utfpr'\n",
            " 'anexo da resolução cogep / utfpr nº'\n",
            " 'anexo da resolução cogep / utfpr nº 110'\n",
            " 'anexo da resolução cogep / utfpr nº 110 de']\n",
            "['anexo', 'da', 'resolução', 'cogep', '/', 'utfpr', 'nº', '110', 'de', '19']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "\n",
        "pln = spacy.load(\"pt_core_news_sm\", disable=[\n",
        "    \"morphologizer\", \"senter\", \"attribute_ruler\", \"ner\"])\n",
        "\n",
        "window_size = 30\n",
        "\n",
        "X = []\n",
        "labels = []\n",
        "\n",
        "for text in corpus:\n",
        "  doc = list(pln(text))\n",
        "  tokens = [ token.text for token in doc ]\n",
        "\n",
        "  for i in range(0, len(tokens)-1):\n",
        "    context = tokens[max(i-window_size, 0):i]\n",
        "    label = tokens[i]\n",
        "\n",
        "    X.append(' '.join(context))\n",
        "    labels.append(label)\n",
        "\n",
        "\n",
        "X = np.array(X, dtype=\"object\")\n",
        "print(X.shape)\n",
        "print(X[:10])\n",
        "\n",
        "print(labels[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0F6SyFYCypA"
      },
      "source": [
        "A camada de Softmax da nossa rede no Keras não aceita palavras como classes de saída. Por isso vamos precisar codificar a nossa saída, utilizando o padrão One-Hot-Encoding, como um vetor com N valores. Sendo que cada um desses N valores representa uma palavra que o nosso modelo pode gerar como saída.\n",
        "\n",
        "A biblioteca Keras disponibiliza a função to_categorical que facilita a implementação dessa conversão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n325APQnlcl8",
        "outputId": "94b50651-4b49-4fb2-83f2-7408b6ade2f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(49520, 2701)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "word_index = list(set(labels))\n",
        "labels_index = [word_index.index(label) for label in labels]\n",
        "y = to_categorical(labels_index)\n",
        "\n",
        "print(y.shape)\n",
        "print(y[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2VqFA4oloDz"
      },
      "source": [
        "##Configuração do Modelo\n",
        "Após definirmos o nosso dataset, nós podemos configurar o nosso modelo de Rede Neural. Especificamente para esse exemplo, vamos definir uma rede com as seguintes camadas:\n",
        "* Vetorização de textos, para gerar as sequências de entrada na rede, conforme vimos na aula passada;\n",
        "* Embedding, contendo as representações vetoriais das palavras do nosso córpus;\n",
        "* Duas camadas de LSTM;\n",
        "* Uma camada Dense;\n",
        "* Uma camada Dense com ativação de tipo Softmax.\n",
        "\n",
        "Nesse modelo, nós utilizamos uma configuração de rede profunda. Observem que dada a maior complexidade do problema a ser tratado, estamos utilizando uma rede também mais complexa. A camada de Embeddings deve utilizar 300 dimensões para representar os tokens; temos duas camadas LSTM com 300 neurônios cada; uma camada Dense também com 300 neurônios; e uma camada Softmax, com 1882 neurônios, um para cada palavra que pode ser gerada pelo nosso modelo.\n",
        "\n",
        "Nessa rede, nós também utilizamos parâmetros de Dropout para reduzir chances de overfitting nas camadas LSTM.\n",
        "\n",
        "Ao compilar o modelo, nós utilizamos uma função de perda diferente, relacionada ao fato de que o nosso não é mais binário, mas considera a possibilidade de múltiplas categorias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "IHkHtjUOlp5X",
        "outputId": "2a104be2-a3d4-4ac9-852d-8a7ff549d9e7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization_3                 │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)                  │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization_3                 │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)                  │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 16ms/step - accuracy: 0.0671 - loss: 6.2629 - val_accuracy: 0.0868 - val_loss: 6.2898\n",
            "Epoch 2/5\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 16ms/step - accuracy: 0.1032 - loss: 5.3736 - val_accuracy: 0.1008 - val_loss: 6.1247\n",
            "Epoch 3/5\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 16ms/step - accuracy: 0.1376 - loss: 4.6897 - val_accuracy: 0.1315 - val_loss: 6.2681\n",
            "Epoch 4/5\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.2079 - loss: 4.0970 - val_accuracy: 0.1430 - val_loss: 6.5153\n",
            "Epoch 5/5\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 16ms/step - accuracy: 0.3103 - loss: 3.4223 - val_accuracy: 0.1708 - val_loss: 6.8629\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d8abadf9510>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from keras.layers import Embedding, LSTM, Dense, TextVectorization\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import AdamW\n",
        "\n",
        "\n",
        "VOCAB_SIZE = 20000\n",
        "MAX_SEQUENCE_SIZE = window_size\n",
        "NEURONS = 300\n",
        "EPOCHS = 5\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "vectorization_layer = TextVectorization(\n",
        "    VOCAB_SIZE, output_sequence_length=MAX_SEQUENCE_SIZE)\n",
        "vectorization_layer.adapt(X)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(vectorization_layer)\n",
        "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM))\n",
        "model.add(LSTM(NEURONS, return_sequences=True))\n",
        "model.add(LSTM(NEURONS))\n",
        "model.add(Dense(NEURONS, activation='relu'))\n",
        "model.add(Dense(len(word_index), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=AdamW(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(X, y, epochs=EPOCHS, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sobre os resultados de treinamento do nosso modelo, é possível observarmos que a acurácia de treinamento e de validação ficaram baixas. Isso pode ser resultado da complexidade do problema de geração de palavras. O nosso modelo pode prever até 1882 palavras, e garantir uma porcentagem elevada de acertos torna-se mais difícil."
      ],
      "metadata": {
        "id": "r9kua38wvtA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em seguida, implementamos uma função de geração de texto. Essa função recebe como entrada o nosso modelo treinado, um conjunto de palavras como entrada, o número de palavras que desejamos gerar, um tamanho máximo da sequência que o nosso modelo suporta e o índice de palavras.\n",
        "\n",
        "O conjunto de palavras dado como entrada deve ser passado para o nosso modelo e usado para gerar palavras que podem ser incluídas ao final da sentença. O nosso modelo gera como saída um conjunto de probabilidades de ocorrências associadas à índices numéricos (Softmax), sendo que o índice numérico com maior probabilidade representa a palavra que o nosso modelo prediz como mais provável de ser incluída no nosso texto. Esses índices são referentes ao índice de palavras (word_index) que pode ser utilizado para transformar o índice numérico em palavra.\n",
        "\n",
        "Toda vez que uma palavra é gerada, nós incluímos essa palavra como entrada para gerar a próxima palavra do nosso texto.\n",
        "\n",
        "Como ainda estamos trabalhando com modelos simples, treinados com poucos parâmetros e poucas épocas, nós configuramos na nossa função a geração de 5 textos distintos entre os mais prováveis que poderiam ser gerados."
      ],
      "metadata": {
        "id": "4TFQHNx0t3-n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPqYEkVKmham",
        "outputId": "4049a9fb-b849-4ba8-c719-85213f7e7b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convalidação é um procedimento para que os alunos possam\n",
            " - aaes de concluído conjunto de editais de extensão de extensão de cursos de graduação e a partir de ser de ser\n",
            " - interveniência do curso de editais de legislação concluído atividades máximo máximo . . . . . . . . . .\n",
            " - cursos de graduação e o prazo máximo máximo estabelecido em atividades . . . . . . . . . .\n",
            " - programas de soma de legislação tenha concluído período letivo . . . . . . . . . . . .\n",
            " - concluído período letivo em atividades de legislação vigente o estudante de cursos de graduação e a partir de partir e a\n",
            "Como atividades completares podem ser realizadas\n",
            " - oficialmente em um professor máximo mínimo de editais de legislação vigente a partir de indicadores profissional de data de partir e\n",
            " - matriculado em cursos de graduação e o estudante de crença de extensão de extensão de cursos de graduação e de cursos\n",
            " - o estágio a ser tenha de concluído conjunto de editais de ser profissional a partir de 1996 a 1996 e de\n",
            " - por acs de ser tenha de concluído conjunto de editais de ser profissional e por meio de partir e de atividades\n",
            " - a modo a ser tenha de concluído conjunto de editais de ser profissional e por meio de partir e de atividades\n",
            "O coeficiente é utilizado para determinar se o aluno\n",
            " - não a flexibilidade de legislação concluído conjunto de ensino e de extensão de partir e de extensão de partir de ser\n",
            " - obrigatório de instituição de extensão de legislação ser de solicitar e de cursos de matrícula em unidades curriculares e o estudante\n",
            " - o especificado parceira o estudante a legislação vigente o disciplina máximo . . . . . . . . . .\n",
            " - de ser tenha parceira a legislação tenha concluído meio de partir e de extensão de partir e de extensão de ser\n",
            " - a ser tenha parceira a legislação tenha concluído meio de partir e de extensão de partir e de extensão de ser\n",
            "Trabalho de Conclusão de Curso pode ser realizado no período\n",
            " - do curso de editais de legislação extensão de extensão de extensão de cursos de matrícula em unidades curriculares e o graduação\n",
            " - em consonância em consonância na modalidade de ead devem ser de ser profissional a partir de 1996 a 1996 a educação\n",
            " - de faltas a legislação programas e registradas de cursos de matrícula em unidades curriculares e docentes a partir e a matrícula\n",
            " - curriculares de instituição de editais de extensão de cursos de graduação e o período letivo para o estudante de ser religião\n",
            " - letivo na modalidade de ead devem prever de cursos de graduação e o período letivo . . . . . .\n",
            "Disciplinas optativas podem ser realizadas no momento em que\n",
            " - não de concluído conjunto de editais de ser profissional a partir de ser profissional a partir de 1996 a 1996 a\n",
            " - obrigatório de legislação concluído conjunto de editais de ser profissional a partir de ser profissional a partir de 1996 a ser\n",
            " - o professor de legislação tenha concluído período letivo . . . . . . . . . . . . .\n",
            " - a legislação tenha concluído conjunto de editais de ser profissional a partir de ser profissional a partir de 1996 a ser\n",
            " - de legislação tenha concluído conjunto de editais de ser profissional a partir de ser profissional a partir de 1996 a ser\n",
            " ----------- \n",
            "art. 1º as atividades acompanhadas caracterizam se pela execução em condições específicas\n",
            " - do estudante de artigos acadêmico de graduação e o estudante de cursos de graduação e a cursos de ead curriculares detalhando\n",
            " - ou a legislação tenha atividades de data de cursos de graduação e de extensão de cursos de graduação e o estudante\n",
            " - da utfpr de legislação vigente o estudante de data de cursos de graduação e de cursos de graduação e o estudante\n",
            " - e a legislação tenha atividades de data de cursos de estágio de extensão de extensão de cursos de graduação de matrícula\n",
            " - de legislação concluído período letivo em atividades máximo . . . . . . . . . . . . §\n",
            "determinando distúrbios agudos ou agudizados caracterizados por incapacidade física relativa incompatível com a frequência às aulas desde que o estudante declare conservar suas condições intelectuais e emocionais necessárias para o\n",
            " - 4º de afastamento de cursos de afastamento acadêmicos período de cargas atividades de matrícula de graduação de matrícula com as atividades\n",
            " - seguintes atividades de extensão e a cursos de graduação de matrícula de graduação de matrícula com as atividades de matrícula com\n",
            " - atividades de extensão e a legislação descritas a seguir dez por cento de matrícula com as atividades de pré requisitos de\n",
            " - unidades curriculares cursadas por curso de graduação de matrícula de graduação de matrícula com as atividades de matrícula com as atividades\n",
            " - período de cargas atividades de matrícula de graduação de matrícula de graduação de matrícula com as atividades de matrícula com as\n"
          ]
        }
      ],
      "source": [
        "def generate_text (model, input, num_words, max_sequence_size, word_index):\n",
        "  outcomes = []\n",
        "\n",
        "  generated_words = []\n",
        "  context = input.split()\n",
        "\n",
        "  diff = max_sequence_size - len(context)\n",
        "  initial_context = ['' for i in range(diff)] + context[-max_sequence_size:]  # left padding\n",
        "  x_test = ' '.join(initial_context).lstrip()\n",
        "\n",
        "  pred = model.predict(np.array([x_test], dtype=\"object\"), verbose=0)\n",
        "  most_probable = [ word_index[i] for i in np.argsort(pred[0])[-5:] ]\n",
        "\n",
        "  print(input)\n",
        "\n",
        "  for next in most_probable:\n",
        "    generated_words = [next]\n",
        "    context = initial_context[1:]\n",
        "    context.append(next)\n",
        "\n",
        "    for i in range(num_words):\n",
        "      x_test = ' '.join(context).lstrip()\n",
        "\n",
        "      pred = model.predict(np.array([x_test], dtype=\"object\"), verbose=0)\n",
        "      next_word = word_index[np.argmax(pred[0])]\n",
        "      generated_words.append(next_word)\n",
        "      context = context[1:]\n",
        "      context.append(next_word)\n",
        "\n",
        "    print(' - ' + ' '.join(generated_words))\n",
        "\n",
        "\n",
        "input = 'Convalidação é um procedimento para que os alunos possam'\n",
        "generate_text(model, input, 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "\n",
        "input = 'Como atividades completares podem ser realizadas'\n",
        "generate_text(model, input, 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "\n",
        "input = 'O coeficiente é utilizado para determinar se o aluno'\n",
        "generate_text(model, input, 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "\n",
        "input = 'Trabalho de Conclusão de Curso pode ser realizado no período'\n",
        "generate_text(model, input, 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "\n",
        "input = 'Disciplinas optativas podem ser realizadas no momento em que'\n",
        "generate_text(model, input, 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "\n",
        "print(' ----------- ')\n",
        "\n",
        "generate_text(model, X[90], 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "generate_text(model, X[242], 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icZK4-mdmwr_"
      },
      "source": [
        "##Embeddings Pré-Treinados e Transfer Learning\n",
        "Com o objetivo de melhorar o processo de geração de texto, nós podemos incluir Embeddings pré-treinados para inicializarmos a camada de Embeddings do nosso modelo. Nesse exemplo, vamos carregar Embeddings pré-treinados com o modelo Skip-Gram de 300 dimensões.\n",
        "\n",
        "Inicialmente, vamos carregar os embeddings pré-treinados em nosso ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evhc17_enNp7",
        "outputId": "284a4cbc-499a-4c27-9503-efee3e9cf5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download concluído e arquivo pronto...\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "# download the embeddings\n",
        "def download_zip (url, emb_filename):\n",
        "  if (os.path.isfile(f'./{emb_filename}')):\n",
        "    print('Arquivo já existente no Runtime... Tudo OK')\n",
        "  else:\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        with zipfile.ZipFile(io.BytesIO(response.content), 'r') as zip_ref:\n",
        "            zip_ref.extractall('./')\n",
        "            print(\"Download concluído e arquivo pronto...\")\n",
        "    else:\n",
        "        print(\"Failed to download the zip file.\")\n",
        "\n",
        "\n",
        "zip_url = \"http://143.107.183.175:22980/download.php?file=embeddings/word2vec/skip_s300.zip\"\n",
        "zip_filename = zip_url.split('/')[-1]\n",
        "emb_filename = zip_filename.replace('.zip', '.txt')\n",
        "download_zip (zip_url, emb_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em seguida, vamos utilizar o vetorizador de textos para gerar o vocabulário de palavras que o nosso corpus utiliza."
      ],
      "metadata": {
        "id": "ybV10cH12m03"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_CVT621oBFo",
        "outputId": "b000bf3c-1e7e-43cc-a730-d8ffe108b29a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2727\n",
            "['', '[UNK]', 'de', 'a', 'art', 'e', 'o', 'do', 'da', 'em', 'no', 'que', 'curso', 'as', 'para', 'ou', 'estudante', 'utfpr', 'atividades', 'curriculares']\n"
          ]
        }
      ],
      "source": [
        "MAX_SIZE_VOCAB = 10000\n",
        "\n",
        "vectorization_layer = TextVectorization(\n",
        "    MAX_SIZE_VOCAB, output_sequence_length=MAX_SEQUENCE_SIZE)\n",
        "vectorization_layer.adapt(corpus)\n",
        "vocab = vectorization_layer.get_vocabulary()\n",
        "\n",
        "print(len(vocab))\n",
        "print(vocab[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tendo o vocabulário definido, vamos, para cada palavra desse vocabulário, carregar seu vetor utilizando a biblioteca Gensim. Com os vetores das palavras, vamos construir a matriz de pesos que deve ser utilizada para inicializar a nossa camada de Embeddings dentro da nossa rede neural.\n",
        "\n",
        "Esses passos são similares aos passos que utilizamos em aulas passadas para carregar os Embeddings."
      ],
      "metadata": {
        "id": "Ld3x0ScF2tAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa57rOg7mxgD",
        "outputId": "9cbadc5e-098a-4054-d745-782214623ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2727, 300)\n",
            "[[ 0.20778722  0.58353364  0.36505863 ...  0.12947296  0.8915747\n",
            "   0.9080447 ]\n",
            " [ 0.861984    0.28851813  0.5303404  ...  0.9152925   0.32804355\n",
            "   0.825434  ]\n",
            " [-0.103755    0.070214   -0.018556   ...  0.043787    0.072865\n",
            "  -0.100664  ]\n",
            " ...\n",
            " [ 0.17013484  0.6039773   0.4264811  ...  0.8406354   0.03417933\n",
            "   0.13831428]\n",
            " [ 0.241192    0.79949486  0.2419448  ...  0.9242134   0.8573463\n",
            "   0.93878764]\n",
            " [ 0.17085135  0.36281216  0.35430688 ...  0.28933045  0.40762407\n",
            "   0.73001516]]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "vectors = KeyedVectors.load_word2vec_format(emb_filename)\n",
        "\n",
        "\n",
        "def get_weight_matrix (vocab, vectors):\n",
        "  weights_matrix = []\n",
        "  _, embedding_dim = vectors.vectors.shape\n",
        "\n",
        "  for word in vocab:\n",
        "    if word in vectors:\n",
        "      weights_matrix.append(vectors[word])\n",
        "    else:\n",
        "      weights_matrix.append(np.random.rand(embedding_dim))\n",
        "\n",
        "\n",
        "  return np.array(weights_matrix, dtype='float32')\n",
        "\n",
        "\n",
        "weights_matrix = get_weight_matrix(vocab, vectors)\n",
        "print(weights_matrix.shape)\n",
        "print(weights_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em seguida, vamos inicializar a matriz de pesos na camada de Embeddings e treinar nosso modelo."
      ],
      "metadata": {
        "id": "6fyetQLY3Hbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9-hpPtHoN8y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "a42c3d9d-492b-4d0d-e91e-2dd563da594a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 14ms/step - accuracy: 0.0795 - loss: 6.1291 - val_accuracy: 0.1068 - val_loss: 6.0816\n",
            "Epoch 2/5\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 13ms/step - accuracy: 0.1164 - loss: 4.9922 - val_accuracy: 0.1339 - val_loss: 6.0057\n",
            "Epoch 3/5\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.2103 - loss: 4.1370 - val_accuracy: 0.1492 - val_loss: 6.0948\n",
            "Epoch 4/5\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.3658 - loss: 3.1676 - val_accuracy: 0.1753 - val_loss: 6.7596\n",
            "Epoch 5/5\n",
            "\u001b[1m1393/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 13ms/step - accuracy: 0.4845 - loss: 2.4720 - val_accuracy: 0.1721 - val_loss: 7.1718\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization_4                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)                  │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │         \u001b[38;5;34m818,100\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │         \u001b[38;5;34m721,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)                 │         \u001b[38;5;34m721,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)                 │          \u001b[38;5;34m90,300\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2701\u001b[0m)                │         \u001b[38;5;34m813,001\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization_4                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)                  │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">818,100</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">90,300</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2701</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">813,001</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,491,405\u001b[0m (36.21 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,491,405</span> (36.21 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,163,801\u001b[0m (12.07 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,163,801</span> (12.07 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m6,327,604\u001b[0m (24.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,327,604</span> (24.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(vectorization_layer)\n",
        "model.add(Embedding(len(vocab), EMBEDDING_DIM, weights=[weights_matrix]))\n",
        "model.add(LSTM(NEURONS, return_sequences=True))\n",
        "model.add(LSTM(NEURONS))\n",
        "model.add(Dense(NEURONS, activation='relu'))\n",
        "model.add(Dense(len(word_index), activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=AdamW(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, y, epochs=EPOCHS, validation_split=0.1)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em seguida, podemos utilizar o modelo treinado para gerar os mesmo textos que foram testados anteriormente."
      ],
      "metadata": {
        "id": "I4euFHX13MqK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBMlxEiRoeEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddecdf83-56ec-41e7-ca6a-749a428a221b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convalidação é um procedimento para que os alunos possam\n",
            " - iv realizado de comissão de caráter escritas ou reitoria de graduação e educação profissional prograd a lei nº de de lei\n",
            " - recorrer à atividades de extensão obrigatório de caráter escritas ou uce a lei nº 9.394 de 20 de dezembro de 1996\n",
            " - analisados pela comissão da realização de ensino e não e seja 01 de intercâmbio de extensão e a lei nº de\n",
            " - disponibilizar atividades complementares ou deverá ser analisados e direta as comunidades externas à universidade tecnológica federal do paraná utfpr . .\n",
            " - ser feita pelo estudante poderá ser analisados e atuando em modalidades do período de intercâmbio internacional e obrigatório e a lei\n",
            "Como atividades completares podem ser realizadas\n",
            " - pelo colegiado de curso de graduação da utfpr devem ser apresentado no ppc deverá estabelecer conforme modelo definidos pelo sistema nacional\n",
            " - por aaes a realização de extensão responsável pelas atividades presencial de pesquisa presencial e turma de extensão e a lei nº\n",
            " - pelos discentes obrigatório de relações 01 acadêmica de intercâmbio de extensão que tecnológica e prorrogação do curso e a lei nº\n",
            " - a realização de extensão obrigatório de caráter escritas ou registradas eletronicamente por meio de requerimento próprio e protocolado no derac em\n",
            " - neste regulamento . . . . . . . . . . . . . . . . . . .\n",
            "O coeficiente é utilizado para determinar se o aluno\n",
            " - que atuando neste regulamento . . . . . . . . . . . . . . . . .\n",
            " - poderá ser apresentado no exercício da lei nº 9.394 de 20 de dezembro de 1996 a lei nº 9.394 de de\n",
            " - estagiário pelas vigor na lei nº 01 de dezembro de administração prévio e rural ou o perfil do egresso dentre as\n",
            " - realizado no exterior não obrigatório de caráter escritas ou produtor opção do estudante encaminhará dias incisos antes do período letivo §\n",
            " - responsável pelo coordenador do curso encaminhará comunicação ao s professor es da s disciplina s em que o estudante encontrar se\n",
            "Trabalho de Conclusão de Curso pode ser realizado no período\n",
            " - profissional definido no sistema acadêmico da utfpr pelos discentes a partir da lei nº 9.394 de 20 de dezembro de 2005\n",
            " - estabelecido nas instruções de matrícula práticas como componente curricular e bem como o turno e a lei nº de de interação\n",
            " - da sua publicação e revoga as versões anteriores e disposições em contrário menor de avaliação do curso e a data de\n",
            " - de intercâmbio descritas no art. 2º incisos i ou i desta resolução quando o período de afastamento do estudante será apresentado\n",
            " - letivo a flexibilidade semestral regular ou mediante autônomo a adequação para o período de data prevista no calendário acadêmico da utfpr\n",
            "Disciplinas optativas podem ser realizadas no momento em que\n",
            " - direta a comissão de caráter escritas ou registradas ausentar se de nível médio de extensão e a lei nº de de\n",
            " - tecnológica que realizado no conselhos do âmbito do curso e e os horários de intercâmbio e não e obrigatório e a\n",
            " - tenha feita neste regulamento . . . . . . . . . . . . . . . . .\n",
            " - realizado em reprovação pela coordenação do curso encaminhará comunicação ao s professor es da s disciplina s em que o estudante\n",
            " - obrigatório de caráter escritas ou registradas eletronicamente por meio de intercâmbio internacional e/ou por prioridades poderá ser definidas a matrícula em\n",
            " ----------- \n",
            "art. 1º as atividades acompanhadas caracterizam se pela execução em condições específicas\n",
            " - em atividades de extensão que creditadas nas seguintes exigências e práticas conforme contribuam de qualquer nível de prova com as exigências\n",
            " - das aaes a diplomação da utfpr a lei nº 9.394 de 20 de dezembro de 1996 a lei nº 11.184 de\n",
            " - que registradas revoga as aaes anteriores e envolver matrícula e a carga horária da extensão e a atividades de ensino e\n",
            " - e a caráter escritas ou uce a lei nº 9.394 de 20 de dezembro de 1996 a lei nº 11.184 de\n",
            " - de atividades de extensão obrigatório de intercâmbio e fundacional de qualquer dos poderes da lei nº de de interação acadêmica acadêmica\n",
            "determinando distúrbios agudos ou agudizados caracterizados por incapacidade física relativa incompatível com a frequência às aulas desde que o estudante declare conservar suas condições intelectuais e emocionais necessárias para o\n",
            " - relatório de ensino . e a data de lei nº de de lei nº de de lei nº de de lei\n",
            " - perfil do egresso . dentre as avaliações de forma de 0 um plano de estágio obrigatório e a data do desenvolvimento\n",
            " - exercício da interação acadêmica de ensino e a data de interação acadêmica de ensino e a data de interação acadêmica de\n",
            " - desenvolvimento da ensino . e a data de lei nº de de lei nº de de lei nº de de lei\n",
            " - representante completa do termo de compromisso de avaliação de estágio obrigatório desde que atendam à área de formação profissional e carga\n"
          ]
        }
      ],
      "source": [
        "input = 'Convalidação é um procedimento para que os alunos possam'\n",
        "generate_text(model, input, 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "\n",
        "input = 'Como atividades completares podem ser realizadas'\n",
        "generate_text(model, input, 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "\n",
        "input = 'O coeficiente é utilizado para determinar se o aluno'\n",
        "generate_text(model, input, 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "\n",
        "input = 'Trabalho de Conclusão de Curso pode ser realizado no período'\n",
        "generate_text(model, input, 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "\n",
        "input = 'Disciplinas optativas podem ser realizadas no momento em que'\n",
        "generate_text(model, input, 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "\n",
        "print(' ----------- ')\n",
        "\n",
        "generate_text(model, X[90], 20, MAX_SEQUENCE_SIZE, word_index)\n",
        "generate_text(model, X[242], 20, MAX_SEQUENCE_SIZE, word_index)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}